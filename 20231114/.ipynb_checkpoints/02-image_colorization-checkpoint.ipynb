{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee21f0c-c6f0-45a2-aa20-db8bbd47b377",
   "metadata": {},
   "source": [
    "# 黑白照片上色\n",
    "\n",
    "https://github.com/open-mmlab/mmagic/tree/main/configs/inst_colorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ecb4d-cff0-4597-817f-5c0707f630b1",
   "metadata": {},
   "source": [
    "## 进入 MMagic 主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051a7e57-5da5-4498-b1e1-f63cb84eab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('mmagic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314c658-c9c3-43d1-b55c-4bf0237774aa",
   "metadata": {},
   "source": [
    "## 下载样例图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0509b658-f1e6-4627-ba4a-814e3a5e4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-14 09:13:15--  https://zihao-openmmlab.obs.cn-east-3.myhuaweicloud.com/20230613-MMagic/data/test_colorization.jpg\n",
      "Connecting to 172.16.0.13:5848... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 30466 (30K) [image/jpeg]\n",
      "Saving to: ‘test_colorization.jpg’\n",
      "\n",
      "test_colorization.j 100%[===================>]  29.75K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-06-14 09:13:15 (564 KB/s) - ‘test_colorization.jpg’ saved [30466/30466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://zihao-openmmlab.obs.cn-east-3.myhuaweicloud.com/20230613-MMagic/data/test_colorization.jpg -O test_colorization.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a794fd-2981-4709-9da5-f349d8a5073b",
   "metadata": {},
   "source": [
    "## 运行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0502fe9b-f3ea-4975-b4b0-9a22bbdeee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-14 09:13:17.633438: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 09:13:18.432670: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /environment/miniconda3/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-06-14 09:13:18.432754: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /environment/miniconda3/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-06-14 09:13:18.432763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmediting/inst_colorization/inst-colorizatioon_full_official_cocostuff-256x256-5b9d4eee.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"demo/mmagic_inference_demo.py\", line 137, in <module>\n",
      "    main()\n",
      "  File \"demo/mmagic_inference_demo.py\", line 132, in main\n",
      "    editor = MMagicInferencer(**vars(args))\n",
      "  File \"/home/featurize/work/MMagic教程/mmagic/mmagic/apis/mmagic_inferencer.py\", line 139, in __init__\n",
      "    self.inferencer = Inferencers(\n",
      "  File \"/home/featurize/work/MMagic教程/mmagic/mmagic/apis/inferencers/__init__.py\", line 54, in __init__\n",
      "    self.inferencer = ColorizationInferencer(\n",
      "  File \"/home/featurize/work/MMagic教程/mmagic/mmagic/apis/inferencers/base_mmagic_inferencer.py\", line 58, in __init__\n",
      "    super().__init__(config, ckpt, device)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/mmengine/infer/infer.py\", line 180, in __init__\n",
      "    self.model = self._init_model(cfg, weights, device)  # type: ignore\n",
      "  File \"/home/featurize/work/MMagic教程/mmagic/mmagic/apis/inferencers/base_mmagic_inferencer.py\", line 76, in _init_model\n",
      "    model.to(device)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py\", line 202, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 899, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 593, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/environment/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 897, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 171.09 MiB already allocated; 16.56 MiB free; 186.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python demo/mmagic_inference_demo.py \\\n",
    "        --model-name inst_colorization \\\n",
    "        --img test_colorization.jpg \\\n",
    "        --result-out-dir out_colorization.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97bdaa-82eb-43ab-a6c8-148da162da1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmagic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8427a9e5acbab940df6c7c884b6e5dca6075b4a5bea483b75e9b99bf8a0b64e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
